{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day 1: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CS 285: Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Lecture 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement Learning vs Standard ML\n",
    "\n",
    "Not iid, hard to label\n",
    "\n",
    "Finite horizon vs infinite horizon RL frameworks\n",
    "\n",
    "Feedback loop conosequesnces (states and rewards) decisions (actions)\n",
    "\n",
    "Examples:\n",
    "    Training a dog\n",
    "    Teachine a robot\n",
    "    Inventory Management (Operations Research):\n",
    "        Net profit as a function of inventory states and update (actions)\n",
    "    Atari Breakout\n",
    "        - Learns complex higher level strategies\n",
    "            - First make path to top, and knock out more points\n",
    "    Robot gripping\n",
    "        - Learns to go in for another grab\n",
    "    Traffic modulation via a single autnomous car\n",
    "\n",
    "Why should we care about 'deep' reinforcement learning\n",
    "\n",
    "How do we 'build' 'intelligent' 'machines?'\n",
    "\n",
    "- Adaptability\n",
    "    - Autonomous oil tanker (easy to navigate, difficuly to fix and diagnose things that go wrong)\n",
    "    - The world is largely unstructured and unpredictable\n",
    "    - Reinforcement learning provides 'a' formalism for behavior (TG - Gammon (90s), AlphaGo, Robotic locamotion, etc.)\n",
    "\n",
    "What is deep RL in particular, and why should we care?\n",
    "\n",
    "Recall an illustrative example from CV: manual feature extraction from HOG features, vs custom learned (optimized for vision by construction) features (CNNs) or more generically (transformers)\n",
    "    (End to end training, but not end to end learning)\n",
    "\n",
    "What does end-to-end learning mean for sequential decision making?\n",
    "\n",
    "What does it mean to have end to end learning?\n",
    "    - Recognition and control part of the learning seperately (with a connecting pipeline). In this setting the control and perception systems are not informed with regards to\n",
    "    the consequenses or demands of each other. If you are designing a system to run away from a tiger, such a system would be problematic (inspiration from nature / for understanding or improving\n",
    "    upon biological systems)\n",
    "\n",
    "    - Alternatively, an end to end system closes the sensory modal loop (informing of both behavioral and perceptual features of the system)\n",
    "    - More over (in a robotics control pipeline, etc) a system can make mistakes at each stage, but are corrected through end to end evaluation.\n",
    "    - Traditional pipeline: observations, state estimation (vision), modeling and prediction, planning, low-level control, controls\n",
    "    - RL (the formalism), DL (the representations)\n",
    "    \n",
    "Why should we study this now (see sited papers)?\n",
    "    - Curriculum learning\n",
    "    - Assisted Learning (curriculumn learning)\n",
    "    - Heirarchical learning (learning heirarchically in general, a curriculum, or a totally different approach)\n",
    "    - Non-Markovian environments\n",
    "    - Experience Replay\n",
    "    - Naturally integrated with Deep NN to obtain high quality generalizations\n",
    "\n",
    "In common RL contexts our ability to 'manually define / train features is significantly limited.'\n",
    "\n",
    "Example Successes:\n",
    "    Q-Learning\n",
    "    Policy Gradients\n",
    "    Guided Policy Search\n",
    "    Supervised learning + policy gradients + value functions + Montecarlo Tree Search\n",
    "    \n",
    "\n",
    "Basic Topics\n",
    "\n",
    "More Advanced Examples (we will cover):\n",
    "    Learning reward functions from examples (inverse reinforcement learning)\n",
    "    Domain transfer (meta-learning, transfer learning)\n",
    "    Learn to predict and to act on prediction\n",
    "\n",
    "Where do rewards come from?\n",
    "\n",
    "Easy to define atari rewards but how about opening a bottle?\n",
    "A paper stated in Berkeley: As human agents we are accustomed to operating with rewards that are so sparse that we only experience them once or twice in a lifetime if at all.\n",
    "    Reddit reply (I pity the author)\n",
    "    \n",
    "    Example: Getting a graduate degree (can't be a direct inference. ... Only do it once (usually))\n",
    "\n",
    "Rewards (/loss) vs objectives\n",
    "\n",
    "Other forms of 'supervision:'\n",
    "\n",
    "Learning from demonstrations:\n",
    "\n",
    "Learning from observing the world\n",
    "\n",
    "Learning from other tasks\n",
    "\n",
    "                                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
